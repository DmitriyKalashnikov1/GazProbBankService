{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb2192f",
   "metadata": {},
   "source": [
    "# Ноутбук для тестов методов кластеризации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b83d24",
   "metadata": {},
   "source": [
    "## 1) Метод на основе CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3cad4",
   "metadata": {},
   "source": [
    "### Суть: векторизуем тесты обращения с помощью стандартного метода CountVectorizer из библиотеки sklearn с учетом би-грамм и три-грамм, вручную находим неободимые сочитания слов и соответствующие им индексы в векторе, разбиваем на наибольшее число кластеров, а затем пишем функцию которая автоматически векторизует входной текст и по наличию или отсутствию выбранных индексов соотносит текст к кластерам\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8bc31",
   "metadata": {},
   "source": [
    "### пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c6de2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: word or n-gramm: and\n",
      "Index 1: word or n-gramm: and this\n",
      "Index 2: word or n-gramm: and this is\n",
      "Index 3: word or n-gramm: document\n",
      "Index 4: word or n-gramm: document is\n",
      "Index 5: word or n-gramm: document is the\n",
      "Index 6: word or n-gramm: first\n",
      "Index 7: word or n-gramm: first document\n",
      "Index 8: word or n-gramm: is\n",
      "Index 9: word or n-gramm: is the\n",
      "Index 10: word or n-gramm: is the first\n",
      "Index 11: word or n-gramm: is the second\n",
      "Index 12: word or n-gramm: is the third\n",
      "Index 13: word or n-gramm: is this\n",
      "Index 14: word or n-gramm: is this the\n",
      "Index 15: word or n-gramm: one\n",
      "Index 16: word or n-gramm: second\n",
      "Index 17: word or n-gramm: second document\n",
      "Index 18: word or n-gramm: the\n",
      "Index 19: word or n-gramm: the first\n",
      "Index 20: word or n-gramm: the first document\n",
      "Index 21: word or n-gramm: the second\n",
      "Index 22: word or n-gramm: the second document\n",
      "Index 23: word or n-gramm: the third\n",
      "Index 24: word or n-gramm: the third one\n",
      "Index 25: word or n-gramm: third\n",
      "Index 26: word or n-gramm: third one\n",
      "Index 27: word or n-gramm: this\n",
      "Index 28: word or n-gramm: this document\n",
      "Index 29: word or n-gramm: this document is\n",
      "Index 30: word or n-gramm: this is\n",
      "Index 31: word or n-gramm: this is the\n",
      "Index 32: word or n-gramm: this the\n",
      "Index 33: word or n-gramm: this the first\n",
      "(4, 34)\n",
      "\n",
      "Text: Is this the second document?\n",
      "Raw vectorizer output:   (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 32)\t1\n",
      "Raw vectorizer output type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "BOW: [0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3)) # создаем объект векторайзера\n",
    "\n",
    "X = vectorizer.fit_transform(corpus) # обучаем его на тексте\n",
    "\n",
    "# Смотрим на индексы комбинаций слов\n",
    "features = vectorizer.get_feature_names_out()\n",
    "for i, f in enumerate(features):\n",
    "    print(f\"Index {i}: word or n-gramm: {f}\")\n",
    "print(X.shape, end=\"\\n\\n\")\n",
    "\n",
    "test = vectorizer.transform(['Is this the second document?'])\n",
    "print(\"Text: Is this the second document?\")\n",
    "print(f\"Raw vectorizer output: {test}\")\n",
    "print(f\"Raw vectorizer output type: {type(test)}\")\n",
    "print(f\"BOW: {test[0].toarray()[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bef4044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW: [[0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# save and restore trained vectorizer\n",
    "import joblib\n",
    "\n",
    "# Save\n",
    "joblib.dump(vectorizer, \"../data/testCountVect.joblib\")\n",
    "\n",
    "# load\n",
    "loadedVect = joblib.load(\"../data/testCountVect.joblib\")\n",
    "\n",
    "#test\n",
    "test = loadedVect.transform(['Is this the second document?'])\n",
    "print(f\"BOW: {test.toarray()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56e8236c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # пример итоговой функции кластеризации по методу 1\n",
    "def clusterisation(texts: list, pathToSavedVectorizer: str):\n",
    "    loadedVect = joblib.load(pathToSavedVectorizer)\n",
    "    \n",
    "    bows = loadedVect.transform(texts)\n",
    "    \n",
    "    conditions = [\n",
    "        lambda x: 1 if x[6] == 1 else 0, # cluster with word 'first'\n",
    "        lambda x: 1 if x[16] == 1 else 0, #cluster with word 'second'\n",
    "        lambda x: 1 if x[25] == 1 else 0, #cluster with word 'third'\n",
    "        lambda x: 1 if not ((x[6] == 1) or (x[16] == 1) or (x[25] == 1)) else 0 # cluster \"Not detect\"\n",
    "    ]\n",
    "    \n",
    "    multiLabel = []\n",
    "    \n",
    "    for f in range(bows.shape[0]):\n",
    "        vect = bows[f].toarray()[0]\n",
    "        labels = [condition(vect) for condition in conditions]\n",
    "        multiLabel += [labels]\n",
    "    return np.array(multiLabel)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8ad865d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 1 0 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"the is first document\", \"second document\", \"first and second document\", \"document\"]\n",
    "\n",
    "labels = clusterisation(texts, \"testCountVect.joblib\")\n",
    "print(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e9c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
